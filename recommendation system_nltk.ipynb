{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4126,"status":"ok","timestamp":1678892007049,"user":{"displayName":"Jenn Lee","userId":"00212290065745703805"},"user_tz":-540},"id":"8F8n73FikZrl","outputId":"d726d6d3-1999-4350-a490-2a7f464e2055"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import json, os\n","import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"hCusrJRQJIp1"},"source":["##Business Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4070,"status":"ok","timestamp":1678892011116,"user":{"displayName":"Jenn Lee","userId":"00212290065745703805"},"user_tz":-540},"id":"V_77bXI-O7PA","outputId":"67c360f5-1bad-444c-a7ae-b0462d28bb8d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(185289, 19)"]},"metadata":{},"execution_count":2}],"source":["df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/project/3rd project_recommender system/data preparation/merged_df.csv')\n","df.shape"]},{"cell_type":"code","source":["df = df.sample(n=50000, random_state=42)\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":617},"id":"troDQlitJHmS","executionInfo":{"status":"ok","timestamp":1678892011116,"user_tz":-540,"elapsed":21,"user":{"displayName":"Jenn Lee","userId":"00212290065745703805"}},"outputId":"a326ebad-e4b6-4208-e79e-74b3036a2c9e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                   business_id                         name             city  \\\n","155376  cGX-1IUwXOjkUqZbkKYcjw                 Fogo de Chao     Philadelphia   \n","14336   Ucl9Vo5lwrUmYbV8Dv8X5g           O'Briens Irish Pub            Tampa   \n","117735  V7TaLZ5EB94hqRIZ1_xjGQ        Kawa Japanese Cuisine      Springfield   \n","69016   cXSyVvOr9YRN9diDkaWs0Q            Honey's Sit-N-Eat     Philadelphia   \n","36614   cGa8ySpfmtfQKPkHJKHnUw  Michael's Family Restaurant  Montgomeryville   \n","\n","       state  stars_x  review_count_x  \\\n","155376    PA      4.0            1426   \n","14336     FL      4.0             108   \n","117735    PA      4.5             145   \n","69016     PA      4.0            1396   \n","36614     PA      3.0              81   \n","\n","                                               categories  \\\n","155376       Seafood, Steakhouses, Brazilian, Restaurants   \n","14336   Bars, Pubs, Nightlife, Arts & Entertainment, I...   \n","117735  Sushi Bars, Restaurants, Ramen, Steakhouses, J...   \n","69016       Southern, Restaurants, American (Traditional)   \n","36614   Restaurants, Breakfast & Brunch, Diners, Sandw...   \n","\n","                       user_id  \\\n","155376  fJ3iKa2YmdNMOOy4L_R9kQ   \n","14336   SZUMrdlgShESvpC2fTgWhg   \n","117735  u4ML5OJou_ShyAEfXA3-YQ   \n","69016   lgUKLRsqvhu4DbM9syzq_Q   \n","36614   GQJn9zM03Yt3ynn2VvKltw   \n","\n","                                                     text  stars_y  useful_x  \\\n","155376  i visited fogo de chao during restaurant week ...        5         0   \n","14336   if you ever want to go where everyone knows yo...        5         0   \n","117735  great service, atmosphere and exceptional sush...        5         6   \n","69016   ok i get it. i understand why people wait ridi...        4         5   \n","36614   i've been to michael's a couple of times now. ...        5         3   \n","\n","        funny_x  cool_x  review_count_y  elite_year_count  activeness  \\\n","155376        0       0              22                 0         0.0   \n","14336         0       0              79                 2        26.0   \n","117735        1       3              12                 0         0.0   \n","69016         3       2             388                 7       302.0   \n","36614         0       1             117                 0         0.0   \n","\n","        useful_y  funny_y  cool_y  \n","155376        14        8       9  \n","14336         64       20       8  \n","117735        10        1       5  \n","69016       1309      510     818  \n","36614        159       24      65  "],"text/html":["\n","  <div id=\"df-3ee870ad-1be1-4a96-9e69-c73299fe04ab\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>business_id</th>\n","      <th>name</th>\n","      <th>city</th>\n","      <th>state</th>\n","      <th>stars_x</th>\n","      <th>review_count_x</th>\n","      <th>categories</th>\n","      <th>user_id</th>\n","      <th>text</th>\n","      <th>stars_y</th>\n","      <th>useful_x</th>\n","      <th>funny_x</th>\n","      <th>cool_x</th>\n","      <th>review_count_y</th>\n","      <th>elite_year_count</th>\n","      <th>activeness</th>\n","      <th>useful_y</th>\n","      <th>funny_y</th>\n","      <th>cool_y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>155376</th>\n","      <td>cGX-1IUwXOjkUqZbkKYcjw</td>\n","      <td>Fogo de Chao</td>\n","      <td>Philadelphia</td>\n","      <td>PA</td>\n","      <td>4.0</td>\n","      <td>1426</td>\n","      <td>Seafood, Steakhouses, Brazilian, Restaurants</td>\n","      <td>fJ3iKa2YmdNMOOy4L_R9kQ</td>\n","      <td>i visited fogo de chao during restaurant week ...</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>22</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>14</td>\n","      <td>8</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>14336</th>\n","      <td>Ucl9Vo5lwrUmYbV8Dv8X5g</td>\n","      <td>O'Briens Irish Pub</td>\n","      <td>Tampa</td>\n","      <td>FL</td>\n","      <td>4.0</td>\n","      <td>108</td>\n","      <td>Bars, Pubs, Nightlife, Arts &amp; Entertainment, I...</td>\n","      <td>SZUMrdlgShESvpC2fTgWhg</td>\n","      <td>if you ever want to go where everyone knows yo...</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>79</td>\n","      <td>2</td>\n","      <td>26.0</td>\n","      <td>64</td>\n","      <td>20</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>117735</th>\n","      <td>V7TaLZ5EB94hqRIZ1_xjGQ</td>\n","      <td>Kawa Japanese Cuisine</td>\n","      <td>Springfield</td>\n","      <td>PA</td>\n","      <td>4.5</td>\n","      <td>145</td>\n","      <td>Sushi Bars, Restaurants, Ramen, Steakhouses, J...</td>\n","      <td>u4ML5OJou_ShyAEfXA3-YQ</td>\n","      <td>great service, atmosphere and exceptional sush...</td>\n","      <td>5</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>69016</th>\n","      <td>cXSyVvOr9YRN9diDkaWs0Q</td>\n","      <td>Honey's Sit-N-Eat</td>\n","      <td>Philadelphia</td>\n","      <td>PA</td>\n","      <td>4.0</td>\n","      <td>1396</td>\n","      <td>Southern, Restaurants, American (Traditional)</td>\n","      <td>lgUKLRsqvhu4DbM9syzq_Q</td>\n","      <td>ok i get it. i understand why people wait ridi...</td>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>388</td>\n","      <td>7</td>\n","      <td>302.0</td>\n","      <td>1309</td>\n","      <td>510</td>\n","      <td>818</td>\n","    </tr>\n","    <tr>\n","      <th>36614</th>\n","      <td>cGa8ySpfmtfQKPkHJKHnUw</td>\n","      <td>Michael's Family Restaurant</td>\n","      <td>Montgomeryville</td>\n","      <td>PA</td>\n","      <td>3.0</td>\n","      <td>81</td>\n","      <td>Restaurants, Breakfast &amp; Brunch, Diners, Sandw...</td>\n","      <td>GQJn9zM03Yt3ynn2VvKltw</td>\n","      <td>i've been to michael's a couple of times now. ...</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>117</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>159</td>\n","      <td>24</td>\n","      <td>65</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ee870ad-1be1-4a96-9e69-c73299fe04ab')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3ee870ad-1be1-4a96-9e69-c73299fe04ab button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3ee870ad-1be1-4a96-9e69-c73299fe04ab');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["df.shape "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8xJkk4pIzwM","executionInfo":{"status":"ok","timestamp":1678892011117,"user_tz":-540,"elapsed":20,"user":{"displayName":"Jenn Lee","userId":"00212290065745703805"}},"outputId":"706a93f5-b56c-4fd0-872b-624532b2a93b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50000, 19)"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"_LYsJppop0fq"},"source":["## Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uR0UtMr0cMVA","executionInfo":{"status":"ok","timestamp":1678893010455,"user_tz":-540,"elapsed":999354,"user":{"displayName":"Jenn Lee","userId":"00212290065745703805"}},"outputId":"c4345dde-4cdf-413f-c33d-e7453e688992"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, SnowballStemmer\n","import string\n","import re, os\n","\n","# Download NLTK data\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","stemmer = SnowballStemmer('english')\n","\n","def preprocess(text):\n","    # Tokenize the text\n","    tokens = nltk.word_tokenize(text)\n","\n","    # Remove stopwords, punctuation marks, and non-alphanumeric characters, and stem the remaining words\n","    tokens = [stemmer.stem(re.sub(r'[^a-zA-Z]', '', token.lower())) for token in tokens if not token.lower() in nltk.corpus.stopwords.words('english') and not token.lower() in string.punctuation]\n","\n","    return tokens\n","\n","# Tokenize and stem the reviews\n","df['review_tokens'] = df['text'].apply(preprocess)\n","\n","# # Define a translation table to remove punctuation marks\n","# translator = str.maketrans('', '', string.punctuation)\n","\n","# # Tokenize the reviews and remove punctuation marks\n","# df['review_tokens'] = df['text'].apply(lambda x: word_tokenize(x.translate(translator)))\n","\n","# # Remove stop words from the tokenized reviews\n","# df['review_tokens'] = df['review_tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n","\n","# # Perform stemming on the remaining words\n","# df['review_tokens'] = df['review_tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n","\n","# Create a vocabulary of unique words\n","vocabulary = set()\n","for tokens in df['review_tokens']:\n","    vocabulary.update(tokens)\n","vocabulary = sorted(list(vocabulary))"]},{"cell_type":"markdown","metadata":{"id":"P3mszuVHq0Ov"},"source":["This code uses the word_tokenize() function from the nltk.tokenize module to tokenize the reviews, the stopwords corpus from the nltk.corpus module to define a list of stop words, and the PorterStemmer and WordNetLemmatizer classes from the nltk.stem module to perform stemming or lemmatization on the remaining words. It then creates a vocabulary of unique words by iterating over the tokenized reviews and adding all unique words to a set, which is then converted to a sorted list. The result is a new column called \"review_tokens\" that contains a list of preprocessed tokens for each review, and a list called \"vocabulary\" that contains all unique words in the reviews."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CJjT6k3Z-oHo","colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"status":"ok","timestamp":1678893010455,"user_tz":-540,"elapsed":7,"user":{"displayName":"Jenn Lee","userId":"00212290065745703805"}},"outputId":"eb8239c7-b479-4ebb-fc8b-14f96846944d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                     text  \\\n","155376  i visited fogo de chao during restaurant week ...   \n","14336   if you ever want to go where everyone knows yo...   \n","117735  great service, atmosphere and exceptional sush...   \n","69016   ok i get it. i understand why people wait ridi...   \n","36614   i've been to michael's a couple of times now. ...   \n","...                                                   ...   \n","44090   for me, rostizado is all about the rotisserie....   \n","103594  we were staying at the royal sonesta for last ...   \n","933     wow. i have been looking for a local indian pl...   \n","160629  loved this place! my wife and i were in town f...   \n","38653   ehh i'll round up from 3.5 stars. this place w...   \n","\n","                                            review_tokens  \n","155376  [visit, fogo, de, chao, restaur, week, group, ...  \n","14336   [ever, want, go, everyon, know, name, , go, ob...  \n","117735  [great, servic, atmospher, except, sushi, ever...  \n","69016   [ok, get, understand, peopl, wait, ridicul, lo...  \n","36614   [ve, michael, s, coupl, time, first, s, import...  \n","...                                                   ...  \n","44090   [rostizado, rotisseri, love, roast, chicken, b...  \n","103594  [stay, royal, sonesta, last, sever, day, go, r...  \n","933     [wow, look, local, indian, place, nirvana, exc...  \n","160629  [love, place, wife, town, weekend, happen, rit...  \n","38653   [ehh, ll, round, , star, place, alright, nt, k...  \n","\n","[50000 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-ba0768d7-e98d-4415-857e-824388a415d3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>review_tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>155376</th>\n","      <td>i visited fogo de chao during restaurant week ...</td>\n","      <td>[visit, fogo, de, chao, restaur, week, group, ...</td>\n","    </tr>\n","    <tr>\n","      <th>14336</th>\n","      <td>if you ever want to go where everyone knows yo...</td>\n","      <td>[ever, want, go, everyon, know, name, , go, ob...</td>\n","    </tr>\n","    <tr>\n","      <th>117735</th>\n","      <td>great service, atmosphere and exceptional sush...</td>\n","      <td>[great, servic, atmospher, except, sushi, ever...</td>\n","    </tr>\n","    <tr>\n","      <th>69016</th>\n","      <td>ok i get it. i understand why people wait ridi...</td>\n","      <td>[ok, get, understand, peopl, wait, ridicul, lo...</td>\n","    </tr>\n","    <tr>\n","      <th>36614</th>\n","      <td>i've been to michael's a couple of times now. ...</td>\n","      <td>[ve, michael, s, coupl, time, first, s, import...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>44090</th>\n","      <td>for me, rostizado is all about the rotisserie....</td>\n","      <td>[rostizado, rotisseri, love, roast, chicken, b...</td>\n","    </tr>\n","    <tr>\n","      <th>103594</th>\n","      <td>we were staying at the royal sonesta for last ...</td>\n","      <td>[stay, royal, sonesta, last, sever, day, go, r...</td>\n","    </tr>\n","    <tr>\n","      <th>933</th>\n","      <td>wow. i have been looking for a local indian pl...</td>\n","      <td>[wow, look, local, indian, place, nirvana, exc...</td>\n","    </tr>\n","    <tr>\n","      <th>160629</th>\n","      <td>loved this place! my wife and i were in town f...</td>\n","      <td>[love, place, wife, town, weekend, happen, rit...</td>\n","    </tr>\n","    <tr>\n","      <th>38653</th>\n","      <td>ehh i'll round up from 3.5 stars. this place w...</td>\n","      <td>[ehh, ll, round, , star, place, alright, nt, k...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50000 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba0768d7-e98d-4415-857e-824388a415d3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ba0768d7-e98d-4415-857e-824388a415d3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ba0768d7-e98d-4415-857e-824388a415d3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}],"source":["df[['text','review_tokens']]"]},{"cell_type":"code","source":["# import spacy\n","# import string\n","# from spacy.lang.en.stop_words import STOP_WORDS\n","\n","# # Load the English model\n","# nlp = spacy.load('en_core_web_sm')\n","\n","# # Define a set of punctuation marks and stop words\n","# punctuations = string.punctuation\n","# stop_words = set(STOP_WORDS)\n","\n","# # Define a function to tokenize and lemmatize text\n","# import re\n","\n","# def preprocess(text):\n","#     # Create a Doc object from the text\n","#     doc = nlp(text)\n","\n","#     # Remove stop words and punctuation marks, and lemmatize the remaining words\n","#     tokens = [re.sub(r'[^a-zA-Z0-9]', '', token.lemma_.lower()) for token in doc if not token.is_stop and not token.is_punct]\n","\n","#     return tokens\n","\n","# # Tokenize and lemmatize the reviews\n","# df['review_tokens'] = df['text'].apply(preprocess)\n","\n","# # Create a vocabulary of unique words\n","# vocabulary = set()\n","# for tokens in df['review_tokens']:\n","#     vocabulary.update(tokens)\n","# vocabulary = sorted(list(vocabulary))"],"metadata":{"id":"HcucTzUSqn7V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MBOnCqilfHNr"},"source":["I'm using SpaCy's en_core_web_sm model to tokenize and lemmatize the text. I define a function preprocess that takes a text as input, creates a Doc object **using SpaCy, removes stop words and punctuation marks, and lemmatizes the remaining words**. I then apply this function to the text column of merged_df using the apply method. Finally, I create a vocabulary of unique words from the tokenized and lemmatized reviews using a set.\n","\n","--> can't run this cell due to time consuming"]},{"cell_type":"markdown","metadata":{"id":"DuiZa-N3tgrE"},"source":["### converting reviews into numerical representations"]},{"cell_type":"markdown","metadata":{"id":"Mf3SVijTA93S"},"source":["Count-Based Representation\n","\n","> TF-IDF Matrix\n","\n"]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Join the preprocessed text into a single string\n","df['review_text'] = df['review_tokens'].apply(lambda x: ' '.join(x))\n","\n","# Define the vectorizer\n","tfidf = TfidfVectorizer(ngram_range=(1,2), max_df=0.75, min_df=5, max_features=5000)\n","\n","# Fit and transform the reviews\n","tfidf_matrix = tfidf.fit_transform(df['review_text'])\n","\n","# Create DTM\n","dtm_tfidf = pd.DataFrame(tfidf_matrix.todense(), columns=tfidf.get_feature_names_out())\n","\n","# Print the shape of the TF-IDF matrix\n","print(tfidf_matrix.shape)\n","display(dtm_tfidf)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":523},"id":"ns8ui1C8rwLb","executionInfo":{"status":"ok","timestamp":1678893027936,"user_tz":-540,"elapsed":17486,"user":{"displayName":"Jenn Lee","userId":"00212290065745703805"}},"outputId":"8ef8217a-3ee2-4523-d66a-6a796a609a61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(50000, 5000)\n"]},{"output_type":"display_data","data":{"text/plain":["            abl  abl get  absolut  absolut amaz  absolut delici  \\\n","0      0.000000      0.0      0.0           0.0             0.0   \n","1      0.000000      0.0      0.0           0.0             0.0   \n","2      0.000000      0.0      0.0           0.0             0.0   \n","3      0.060096      0.0      0.0           0.0             0.0   \n","4      0.000000      0.0      0.0           0.0             0.0   \n","...         ...      ...      ...           ...             ...   \n","49995  0.000000      0.0      0.0           0.0             0.0   \n","49996  0.000000      0.0      0.0           0.0             0.0   \n","49997  0.000000      0.0      0.0           0.0             0.0   \n","49998  0.000000      0.0      0.0           0.0             0.0   \n","49999  0.000000      0.0      0.0           0.0             0.0   \n","\n","       absolut favorit  absolut love  abund   ac  acai  ...  yolk  york  \\\n","0                  0.0           0.0    0.0  0.0   0.0  ...   0.0   0.0   \n","1                  0.0           0.0    0.0  0.0   0.0  ...   0.0   0.0   \n","2                  0.0           0.0    0.0  0.0   0.0  ...   0.0   0.0   \n","3                  0.0           0.0    0.0  0.0   0.0  ...   0.0   0.0   \n","4                  0.0           0.0    0.0  0.0   0.0  ...   0.0   0.0   \n","...                ...           ...    ...  ...   ...  ...   ...   ...   \n","49995              0.0           0.0    0.0  0.0   0.0  ...   0.0   0.0   \n","49996              0.0           0.0    0.0  0.0   0.0  ...   0.0   0.0   \n","49997              0.0           0.0    0.0  0.0   0.0  ...   0.0   0.0   \n","49998              0.0           0.0    0.0  0.0   0.0  ...   0.0   0.0   \n","49999              0.0           0.0    0.0  0.0   0.0  ...   0.0   0.0   \n","\n","       young  younger  yum  yum yum  yummi  zero  zone  zucchini  \n","0        0.0      0.0  0.0      0.0    0.0   0.0   0.0       0.0  \n","1        0.0      0.0  0.0      0.0    0.0   0.0   0.0       0.0  \n","2        0.0      0.0  0.0      0.0    0.0   0.0   0.0       0.0  \n","3        0.0      0.0  0.0      0.0    0.0   0.0   0.0       0.0  \n","4        0.0      0.0  0.0      0.0    0.0   0.0   0.0       0.0  \n","...      ...      ...  ...      ...    ...   ...   ...       ...  \n","49995    0.0      0.0  0.0      0.0    0.0   0.0   0.0       0.0  \n","49996    0.0      0.0  0.0      0.0    0.0   0.0   0.0       0.0  \n","49997    0.0      0.0  0.0      0.0    0.0   0.0   0.0       0.0  \n","49998    0.0      0.0  0.0      0.0    0.0   0.0   0.0       0.0  \n","49999    0.0      0.0  0.0      0.0    0.0   0.0   0.0       0.0  \n","\n","[50000 rows x 5000 columns]"],"text/html":["\n","  <div id=\"df-faec8ba3-8301-4e95-b932-faf64be2bc9d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abl</th>\n","      <th>abl get</th>\n","      <th>absolut</th>\n","      <th>absolut amaz</th>\n","      <th>absolut delici</th>\n","      <th>absolut favorit</th>\n","      <th>absolut love</th>\n","      <th>abund</th>\n","      <th>ac</th>\n","      <th>acai</th>\n","      <th>...</th>\n","      <th>yolk</th>\n","      <th>york</th>\n","      <th>young</th>\n","      <th>younger</th>\n","      <th>yum</th>\n","      <th>yum yum</th>\n","      <th>yummi</th>\n","      <th>zero</th>\n","      <th>zone</th>\n","      <th>zucchini</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.060096</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>49995</th>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>49996</th>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>49997</th>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>49998</th>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>49999</th>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50000 rows × 5000 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-faec8ba3-8301-4e95-b932-faf64be2bc9d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-faec8ba3-8301-4e95-b932-faf64be2bc9d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-faec8ba3-8301-4e95-b932-faf64be2bc9d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"Thf4gaE3jBb-"},"source":["I've added some additional parameters to the TfidfVectorizer:\n","\n","- ngram_range=(1,2): This specifies that the vectorizer should consider both unigrams and bigrams when creating features.\n","- max_df=0.75: This specifies that words should be excluded from the vocabulary if they appear in more than 75% of the documents.\n","- min_df=5: This specifies that words should be excluded from the vocabulary if they appear in fewer than 5 documents.\n","- max_features=5000: This specifies that the vectorizer should consider at most 5000 features (i.e., the 5000 most frequent words in the corpus)."]},{"cell_type":"markdown","source":["how to split the data into training and test sets, train a recommender system using TF-IDF, generate recommendations for each user in the test set, and calculate the MAP score"],"metadata":{"id":"bailzEacjs0F"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Join the preprocessed text into a single string\n","df['review_text'] = df['review_tokens'].apply(lambda x: ' '.join(x))\n","\n","# Split data into train and test sets\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# Define the vectorizer\n","tfidf = TfidfVectorizer(ngram_range=(1,2), max_df=0.75, min_df=5, max_features=5000)\n","\n","# Fit and transform the reviews\n","tfidf_matrix_train = tfidf.fit_transform(train_df['review_text'])\n","\n","# Initialize cosine similarity matrix\n","cosine_sim = cosine_similarity(tfidf_matrix_train, tfidf_matrix_train)\n","\n","# Define function to get top recommendations for each user\n","def recommendations(user_id, cosine_sim_matrix, df, top_n=5):\n","    # Get index of user_id in df\n","    user_index = df[df['user_id'] == user_id].index[0]\n","\n","    # Get cosine similarity scores for user_index\n","    sim_scores = list(enumerate(cosine_sim_matrix[user_index]))\n","\n","    # Sort the list of sim_scores in descending order\n","    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n","\n","    # Get indices of top_n similar users\n","    top_similar_users = [i[0] for i in sim_scores[1:top_n+1]]\n","\n","    # Get the restaurant recommendations for the top_n similar users\n","    recommended_restaurants = df.iloc[top_similar_users][['name', 'categories', 'stars_x']]\n","\n","    return recommended_restaurants\n","\n","user_id = 'fJ3iKa2YmdNMOOy4L_R9kQ'\n","top_n = 5\n","recommended_restaurants = recommendations(user_id, cosine_sim, df, top_n)"],"metadata":{"id":"gg-PKDu7rCax"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.model_selection import train_test_split\n","\n","# Split data into train and test sets\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# Create TF-IDF vectorizer\n","tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=0.75, min_df=5, max_features=5000)\n","\n","# Fit and transform the training data\n","tfidf_matrix_train = tfidf.fit_transform(train_df['text'])\n","\n","# Initialize cosine similarity matrix\n","cosine_sim = cosine_similarity(tfidf_matrix_train, tfidf_matrix_train)\n","\n","# Get a list of restaurants reviewed by a given user\n","def get_user_reviews(user_id, df):\n","    return df[df['user_id'] == user_id]['business_id'].tolist()\n","\n","# Find similar restaurants for a given restaurant\n","def get_similar_restaurants(restaurant_id, tfidf_matrix, df):\n","    restaurant_idx = df[df['business_id'] == restaurant_id].index[0]\n","    similarity_scores = cosine_similarity(tfidf_matrix[restaurant_idx], tfidf_matrix)\n","    similar_restaurants = list(enumerate(similarity_scores[0]))\n","    similar_restaurants = sorted(similar_restaurants, key=lambda x: x[1], reverse=True)\n","    return similar_restaurants[1:]\n","\n","# Make recommendations for a given user\n","def recommend_restaurants(user_id, tfidf_matrix, df, top_n=5):\n","    user_reviews = get_user_reviews(user_id, df)\n","    restaurant_scores = {}\n","    for restaurant_id in user_reviews:\n","        similar_restaurants = get_similar_restaurants(restaurant_id, tfidf_matrix, df)\n","        for i, (idx, score) in enumerate(similar_restaurants):\n","            if idx in restaurant_scores:\n","                restaurant_scores[idx] += score * (0.95 ** i)\n","            else:\n","                restaurant_scores[idx] = score * (0.95 ** i)\n","    restaurant_scores = sorted(restaurant_scores.items(), key=lambda x: x[1], reverse=True)\n","    recommended_restaurants = [df.iloc[idx]['business_id'] for idx, score in restaurant_scores[:top_n]]\n","    return recommended_restaurants"],"metadata":{"id":"WEQHlmgocgZT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define function to calculate Average Precision (AP) for a given user\n","def apk(actual, predicted, k=10):\n","    if len(predicted) > k:\n","        predicted = predicted[:k]\n","\n","    score = 0.0\n","    num_hits = 0.0\n","\n","    for i, p in enumerate(predicted):\n","        if p in actual and p not in predicted[:i]:\n","            num_hits += 1.0\n","            score += num_hits / (i+1.0)\n","\n","    if not actual:\n","        return 0.0\n","\n","    return score / min(len(actual), k)\n","\n","# Get unique user_ids in test_df\n","users_test = test_df['user_id'].unique()\n","\n","# Initialize list to store AP scores for each user\n","ap_scores = []\n","\n","# Generate recommendations and calculate AP for each user in test_df\n","for user in users_test:\n","    # Get recommendations for user\n","    recommendations = get_recommendations(user, cosine_sim, train_df)\n","\n","    # Get actual restaurants rated by user in test_df\n","    actual = test_df[test_df['user_id'] == user]['name'].tolist()\n","\n","    # Calculate AP for user\n","    ap = apk(actual, recommendations['name'].tolist())\n","\n","    # Append AP score to ap_scores\n","    ap_scores.append(ap)\n","\n","# Calculate MAP score\n","map_score = sum(ap_scores) / len(ap_scores)\n","\n","print(\"MAP Score: \", map_score)"],"metadata":{"id":"olrin9aMlgv4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ypj1rpVuslJ-"},"source":["The main difference between the two is that CountVectorizer simply counts the number of times each word appears in a document, while TfidfVectorizer takes into account the frequency of the word in the entire corpus of documents.\n","\n","TfidfVectorizer gives more weight to words that are more important or informative for a given document, while CountVectorizer treats all words equally.\n","\n","Refer to https://arxiv.org/pdf/2004.13851.pdf\n","\n"]},{"cell_type":"code","source":["user_id = 'your_user_id_here'\n","top_n = 5\n","recommended_restaurants = recommend_restaurants(user_id, tfidf_matrix, df, top_n)"],"metadata":{"id":"FORYS6rocjzz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Split data into training and testing sets\n","train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)"],"metadata":{"id":"1ayD-zWZdp5H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_apk(actual, predicted, k=10):\n","    \"\"\"\n","    Calculates the Average Precision at k for a single user.\n","    \n","    Args:\n","    actual (list): A list of the actual restaurant IDs (as strings).\n","    predicted (list): A list of the predicted restaurant IDs (as strings).\n","    k (int): The number of recommendations to consider.\n","    \n","    Returns:\n","    apk (float): The Average Precision at k for the user.\n","    \"\"\"\n","    if len(predicted) > k:\n","        predicted = predicted[:k]\n","    \n","    score = 0.0\n","    num_hits = 0.0\n","    \n","    for i, p in enumerate(predicted):\n","        if p in actual and p not in predicted[:i]:\n","            num_hits += 1.0\n","            score += num_hits / (i+1.0)\n","            \n","    if not actual:\n","        return 0.0\n","    \n","    return score / min(len(actual), k)\n","\n","def calculate_map(actual_dict, predicted_dict, k=10):\n","    \"\"\"\n","    Calculates the Mean Average Precision for a set of recommendations.\n","    \n","    Args:\n","    actual_dict (dict): A dictionary mapping user IDs to lists of actual restaurant IDs.\n","    predicted_dict (dict): A dictionary mapping user IDs to lists of predicted restaurant IDs.\n","    k (int): The number of recommendations to consider.\n","    \n","    Returns:\n","    map (float): The Mean Average Precision for the recommendations.\n","    \"\"\"\n","    apks = []\n","    for user_id in actual_dict.keys():\n","        actual = actual_dict[user_id]\n","        predicted = predicted_dict.get(user_id, [])\n","        apk = calculate_apk(actual, predicted, k=k)\n","        apks.append(apk)\n","        \n","    return np.mean(apks)"],"metadata":{"id":"rCul31oydmjY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZAnyt33jHKtt"},"outputs":[],"source":["# from keras.models import Sequential, Model\n","# from keras.layers import Dense, Dropout, Activation, Embedding, Conv1D, MaxPooling1D, Flatten, Input, concatenate\n","# from keras.optimizers import Adam\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.linear_model import LogisticRegression\n","# from sklearn.metrics import accuracy_score\n","\n","# # Load the data and split into train and test sets\n","# X = df['text']\n","# y = df['user_id']\n","# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# # Convert the text data to a TF-IDF matrix\n","# X_train_tfidf = tfidf.transform(X_train)\n","# X_test_tfidf = tfidf.transform(X_test)\n","\n","# # Define the deep learning model\n","# input_layer = Input(shape=(X_train_tfidf.shape[1],))\n","# embedding_layer = Embedding(input_dim=X_train_tfidf.shape[1], output_dim=128)(input_layer)\n","# conv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n","# max_pool_layer = MaxPooling1D(pool_size=2)(conv_layer)\n","# flatten_layer = Flatten()(max_pool_layer)\n","# dense_layer = Dense(128, activation='relu')(flatten_layer)\n","# output_layer = Dense(1, activation='sigmoid')(dense_layer)\n","# cnn_model = Model(inputs=input_layer, outputs=output_layer)\n","\n","# # Compile the model\n","# cnn_model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n","\n","# # Train the CNN model\n","# cnn_model.fit(X_train_tfidf, y_train, batch_size=64, epochs=5, validation_data=(X_test_tfidf, y_test))\n","\n","# # Get the predictions from the CNN model\n","# cnn_preds_train = cnn_model.predict(X_train_tfidf)\n","# cnn_preds_test = cnn_model.predict(X_test_tfidf)\n","\n","# # Define the traditional machine learning model\n","# lr_model = LogisticRegression()\n","\n","# # Train the traditional machine learning model on the TF-IDF matrix\n","# lr_model.fit(X_train_tfidf, y_train)\n","\n","# # Get the predictions from the traditional machine learning model\n","# lr_preds_train = lr_model.predict_proba(X_train_tfidf)[:,1]\n","# lr_preds_test = lr_model.predict_proba(X_test_tfidf)[:,1]\n","\n","# # Concatenate the predictions from both models\n","# hybrid_train = np.concatenate((cnn_preds_train, lr_preds_train.reshape(-1,1)), axis=1)\n","# hybrid_test = np.concatenate((cnn_preds_test, lr_preds_test.reshape(-1,1)), axis=1)\n","\n","# # Define the final classifier\n","# final_model = LogisticRegression()\n","\n","# # Train the final classifier on the concatenated predictions\n","# final_model.fit(hybrid_train, y_train)\n","\n","# # Evaluate the final model on the test set\n","# final_preds = final_model.predict(hybrid_test)\n","# accuracy = accuracy_score(y_test, final_preds)\n","# print('Accuracy: {}'.format(accuracy))"]},{"cell_type":"markdown","metadata":{"id":"m8-qNaUTMOxZ"},"source":["two classes: \"liked by the user\" & \"not liked by the user\"<br/>\n","-->Binary classification prob."]},{"cell_type":"markdown","metadata":{"id":"IAtpsfOCBW6U"},"source":["Distributed-Based Representation\n","\n","\n","> Embedding Matrix: Pretrained word embedding GloVe\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WR3lkiGG3eN"},"outputs":[],"source":["# from keras.preprocessing.text import Tokenizer\n","# from tensorflow.keras.preprocessing.sequence import pad_sequences\n","# from keras.models import Sequential\n","# from keras.layers import Embedding, LSTM, Dense\n","\n","# # Use pre-trained GloVe embeddings\n","# embeddings_index = {}\n","# embedding_file = '/Users/username/Documents/glove.6B.100d.txt\n","# with open(embedding_file, 'r', encoding='utf-8') as f:\n","#     for line in f:\n","#         values = line.split()\n","#         word = values[0]\n","#         coefs = np.asarray(values[1:], dtype='float32')\n","#         embeddings_index[word] = coefs\n","\n","# # Convert text data to sequences of word indices\n","# tokenizer = Tokenizer(num_words=5000)\n","# tokenizer.fit_on_texts(X_train)\n","# X_train_sequences = tokenizer.texts_to_sequences(X_train)\n","# X_test_sequences = tokenizer.texts_to_sequences(X_test)\n","\n","# # Pad sequences to a fixed length\n","# max_sequence_length = 100\n","# X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n","# X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n","\n","# # Create embedding matrix\n","# word_index = tokenizer.word_index\n","# num_words = min(5000, len(word_index) + 1)\n","# embedding_matrix = np.zeros((num_words, 100))\n","# for word, i in word_index.items():\n","#     if i >= num_words:\n","#         continue\n","#     embedding_vector = embeddings_index.get(word)\n","#     if embedding_vector is not None:\n","#         embedding_matrix[i] = embedding_vector\n","\n","# # Build deep learning model\n","# model = Sequential()\n","# model.add(Embedding(input_dim=num_words, output_dim=100, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n","# model.add(LSTM(64))\n","# model.add(Dense(10, activation='softmax'))\n","# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# # Train model on training data\n","# model.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_split=0.2)\n","\n","# # Evaluate model on test data\n","# score = model.evaluate(X_test_padded, y_test, batch_size=32)\n","\n","# # Make restaurant recommendations\n","# user_preferences = 'I want a cheap Italian restaurant near downtown'\n","# user_preferences = ' '.join([stemmer.stem(token) for token in tokenizer(user_preferences.lower())])\n","# user_sequence = tokenizer.texts_to_sequences([user_preferences])\n","# user_padded = pad_sequences(user_sequence, maxlen=max_sequence_length)\n","# predicted_ratings = model.predict(user_padded)\n","# top_restaurants = df.loc[df['stars'].isin(np.argsort(predicted_ratings)[-10:])]"]},{"cell_type":"markdown","metadata":{"id":"wWYHPt-dHUzb"},"source":["This code uses GloVe pre-trained word embeddings to create an embedding matrix, and builds a deep learning model using LSTM layers to predict restaurant ratings. It then trains the model on the training data, evaluates the model on the test data, and makes restaurant recommendations based on the user's preferences"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61972,"status":"ok","timestamp":1678867194115,"user":{"displayName":"Jenn Lee","userId":"00212290065745703805"},"user_tz":-540},"id":"qIij-LaRKEOO","outputId":"6fa27e16-514b-49a8-e417-9e0de725b896"},"outputs":[{"name":"stdout","output_type":"stream","text":["[==================================================] 100.0% 128.1/128.1MB downloaded\n"]}],"source":["import gensim.downloader as api\n","\n","# Download and load the pre-trained GloVe word embeddings\n","word_vectors = api.load(\"glove-wiki-gigaword-100\")"]},{"cell_type":"markdown","metadata":{"id":"ntyvv9rx8TGT"},"source":["Gensim is an open-source Python library designed to process and analyze large-scale collections of text data. It provides implementations of several state-of-the-art algorithms for natural language processing (NLP), including topic modeling, document similarity analysis, and word embedding.\n","\n","GloVe (Global Vectors for Word Representation) is a popular unsupervised algorithm for generating word embeddings, which are dense vector representations of words that capture their semantic and syntactic meaning. The GloVe algorithm is typically used to train word embeddings on large text corpora.\n","\n","While GloVe itself is not directly integrated into Gensim, Gensim provides an interface for loading pre-trained GloVe embeddings and using them for downstream NLP tasks. This makes it easy to incorporate GloVe embeddings into your own NLP models built with Gensim."]},{"cell_type":"markdown","metadata":{"id":"qpCc4ye_pHEl"},"source":["## Splitting the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C-SfBRGeskxV"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Split data into training and testing sets\n","train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"o3t4juWxpJoV"},"source":["## Model Selection\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JFstSCqeLk7G"},"outputs":[],"source":["from keras.preprocessing.text import Tokenizer\n","\n","# Create a tokenizer\n","tokenizer = Tokenizer()\n","\n","# Fit the tokenizer on the list of review tokens\n","tokenizer.fit_on_texts(df['review_tokens'])\n","\n","# Convert the list of tokenized reviews to sequences\n","sequences = tokenizer.texts_to_sequences(df['review_tokens'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uAr7tc_S6LC2"},"outputs":[],"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","import gc\n","\n","nltk.download('vader_lexicon')\n","\n","# Initialize the VADER sentiment analyzer\n","sid = SentimentIntensityAnalyzer()\n","\n","# Define a function to convert each review to a sequence of vectors\n","def reviews_to_vectors_with_sentiment(reviews, tokenizer, max_length, sid, embedding_model):\n","    # Convert the tokenized reviews to sequences of word indices\n","    sequences = tokenizer.texts_to_sequences(reviews)\n","\n","    # Convert the sequences of word indices to sequences of word embeddings\n","    vectors = []\n","    for seq in sequences:\n","        vector_seq = []\n","        for word_index in seq:\n","            word = tokenizer.index_word[word_index]\n","            if word in embedding_model:\n","                vector_seq.append(embedding_model[word])\n","        vectors.append(vector_seq)\n","\n","    # Pad the sequences with zeros so that all reviews have the same length\n","    padded_vectors = pad_sequences(vectors, maxlen=max_length, padding='post')\n","\n","    # Calculate sentiment scores for each review\n","    sentiment_scores = []\n","    for i, review in enumerate(reviews):\n","        ss = sid.polarity_scores(review)\n","        sentiment_scores.append(ss['compound'])\n","        if i % 1000 == 0:\n","            gc.collect()  # Call the garbage collector every 1000 reviews to free up memory\n","    \n","    sentiment_scores = np.array(sentiment_scores)\n","    return padded_vectors, sentiment_scores\n","\n","# Convert the training and testing data to sequences of vectors\n","max_length = 100  # Set the maximum length of a review to 100 words\n","train_vectors, train_sentiment = reviews_to_vectors_with_sentiment(train_data['text'], tokenizer, max_length, sid, word_vectors)\n","test_vectors, test_sentiment = reviews_to_vectors_with_sentiment(test_data['text'], tokenizer, max_length, sid, word_vectors)\n","\n","# Split the data into input and output arrays\n","X_train = train_vectors\n","y_train = train_sentiment\n","X_test = test_vectors\n","y_test = test_sentiment"]},{"cell_type":"markdown","metadata":{"id":"jrSQsPKwbH_w"},"source":["I can use unsupervised sentiment analysis techniques such as sentiment lexicons to estimate the sentiment of your reviews. A sentiment lexicon is a collection of words and their associated sentiment scores (e.g., positive, negative, or neutral).\n","\n","The sentiment_scores list returned by the reviews_to_vectors_with_sentiment function is a list of dictionaries, which cannot be directly encoded by the LabelEncoder or any other encoder."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_RWDESRHMa0s"},"outputs":[],"source":["# # Build deep learning model\n","# model = Sequential()\n","# model.add(Embedding(input_dim=num_words, output_dim=100, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n","# model.add(LSTM(64))\n","# model.add(Dense(10, activation='softmax'))\n","# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# # Train model on training data\n","# model.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_split=0.2)\n","\n","# # Evaluate model on test data\n","# score = model.evaluate(X_test_padded, y_test, batch_size=32)\n","\n","# # Make restaurant recommendations\n","# user_preferences = 'I want a cheap Italian restaurant near downtown'\n","# user_preferences = ' '.join([stemmer.stem(token) for token in tokenizer(user_preferences.lower())])\n","# user_sequence = tokenizer.texts_to_sequences([user_preferences])\n","# user_padded = pad_sequences(user_sequence, maxlen=max_sequence_length)\n","# predicted_ratings = model.predict(user_padded)\n","# top_restaurants = df.loc[df['stars'].isin(np.argsort(predicted_ratings)[-10:])]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKbqKiOvjCQU"},"outputs":[],"source":["print(train_vectors.shape)\n","print(y_train.shape)\n","print(train_vectors.dtype)\n","print(y_train.dtype)"]},{"cell_type":"markdown","metadata":{"id":"bOWT37Px9jBp"},"source":["## Model Training\n","\n","I will train the deep neural network using the training set and validate it using the testing set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKVN9-kt80FG"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n","from tensorflow.keras.models import Sequential\n","\n","# Define the RNN model architecture\n","model = Sequential()\n","model.add(Embedding(input_dim=len(word2vec_model.wv.vocab), output_dim=100, weights=[tfidf], input_length=max_length))\n","model.add(SimpleRNN(units=128))\n","model.add(Dense(units=1, activation='sigmoid'))"]},{"cell_type":"markdown","metadata":{"id":"RIr8Zj2d847k"},"source":["This code defines a simple RNN model with an Embedding layer, a SimpleRNN layer, and a Dense output layer. The Embedding layer is initialized with the pre-trained word vectors, and the weights are fixed during training. The SimpleRNN layer processes the sequence of word vectors and produces a final output, which is passed through a Dense layer with a sigmoid activation function to produce a binary classification output.\n","\n","I chose the RNN model with an Embedding layer, a SimpleRNN layer, and a Dense output layer because it is a popular and effective model for processing sequential data like text. The Embedding layer is used to convert the sequence of words represented by the pre-trained word embeddings into a sequence of dense vectors that can be understood by the RNN model. The SimpleRNN layer is used to process the sequence of dense vectors and capture the relationships between the words in the sequence. Finally, the Dense output layer is used to predict the sentiment of the restaurant review based on the relationships between the words captured by the RNN model.\n","\n","The RNN model is a good choice for this problem because it can capture the contextual relationships between words in a sentence. It is well-suited for text classification tasks like sentiment analysis, where the order of words in a sentence is important for determining the sentiment of the sentence. Additionally, the SimpleRNN layer is a computationally efficient choice for this problem, as it is capable of capturing short-term dependencies in the input sequence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9MHVn4-9j_Z"},"outputs":[],"source":["# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(train_vectors, train_labels, epochs=10, batch_size=64, validation_data=(test_vectors, test_labels))\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(test_vectors, test_labels, verbose=False)\n","print(f'Test accuracy: {accuracy:.3f}')"]},{"cell_type":"markdown","metadata":{"id":"uiLj7aHV9mB4"},"source":["The model is then compiled with an Adam optimizer and binary cross-entropy loss function.\n","\n","For binary classification problems, where the goal is to classify an input into one of two classes, binary cross-entropy is a common loss function used in neural networks. It calculates the difference between the predicted probabilities and the true class labels and updates the model weights accordingly. It is often chosen because it has desirable mathematical properties and has been shown to work well in practice.\n","\n","As for the metrics, accuracy is a commonly used metric for classification problems. It measures the proportion of correctly classified examples out of all the examples. In this case, it tells us the percentage of reviews that are correctly classified as positive or negative. By using accuracy as the metric, we can easily evaluate the performance of the model and compare it to other models that use the same metric.\n","\n","It is trained on the train_vectors and train_labels arrays for 10 epochs with a batch size of 64. Finally, the model is evaluated on the test_vectors and test_labels arrays, and the test accuracy is printed."]},{"cell_type":"markdown","metadata":{"id":"rLn_G8OF6BiJ"},"source":["model selection not for deep learning\n","\n","**Collaborative Filtering** recommends items based on similar users' preferences.<br>\n","**Content-based Filtering** recommends items based on the attributes of the items.<br>\n","**Hybrid Filtering** combines both Collaborative and Content-based filtering to improve the performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xsQLX3e2xuKU"},"outputs":[],"source":["# !pip install scikit-surprise"]},{"cell_type":"markdown","metadata":{"id":"UnrTw0Wryx2B"},"source":["- I need to first create the necessary data structures to store the required information.\n","\n","1. **Collaborative Filtering**: \n","I will use the Surprise library to implement collaborative filtering. Surprise is a Python library for building and analyzing recommender systems that deal with explicit rating data.\n","\n","2. **Content-based Filtering**:\n","I will use the TF-IDF matrix that I created earlier. I can compute the pairwise cosine similarity between all the items using the cosine_similarity function from scikit-learn.\n","\n","3. **Hybrid Filtering**:\n","Once I have the necessary data structures, I can use them to build a hybrid recommendation system. The basic idea behind hybrid filtering is to combine the predictions from collaborative and content-based filtering models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lNPKYOwNxufV"},"outputs":[],"source":["# Collaborative Filtering\n","from surprise import Reader, Dataset, SVD\n","from surprise.model_selection import cross_validate\n","\n","# Define a reader to read in the rating data\n","reader = Reader(rating_scale=(1, 5))\n","\n","# Load the rating data into a Surprise dataset\n","data = Dataset.load_from_df(df[['user_id', 'business_id', 'stars_y']], reader)\n","\n","# Define a collaborative filtering model using SVD\n","model_collab = SVD()\n","\n","# Train the model on the rating data\n","model_collab.fit(data)"]},{"cell_type":"markdown","metadata":{"id":"frQNrQjeyUb7"},"source":["SVD can be used to perform Latent Semantic Analysis (LSA) in NLP by transforming a document-term matrix into a document-concept matrix, where the concepts are derived from the singular values and vectors of the document-term matrix. This can be useful for text classification, information retrieval, and topic modeling.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lSCR0FUwx5bx"},"outputs":[],"source":["# Content-based Filtering\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Compute the pairwise cosine similarity between all the items\n","cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"]},{"cell_type":"markdown","metadata":{"id":"1VaviWnK121k"},"source":["Here, I will use a simple linear combination of the two models' predictions, where the weight given to each model is determined by a parameter alpha."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8h5TnGrfykf0"},"outputs":[],"source":["# Hybrid Filtering\n","def hybrid_recommendations(user_id, business_id, alpha=0.5):\n","    \"\"\"\n","    Get hybrid recommendations for a user and a business.\n","    \n","    Parameters:\n","        - user_id: ID of the user for whom to make recommendations\n","        - business_id: ID of the business for which to make recommendations\n","        - alpha: Weight given to the collaborative filtering model (default: 0.5)\n","    \n","    Returns:\n","        - A list of top 10 recommended business names and their corresponding predicted ratings\n","    \"\"\"\n","    \n","    # Get the indices of the user and business in the data matrix\n","    user_idx = np.where(df['user_id'] == user_id)[0][0]\n","    business_idx = np.where(df['business_id'] == business_id)[0][0]\n","    \n","    # Compute the collaborative filtering prediction\n","    collab_pred = model_collab.predict(user_id, business_id).est\n","    \n","    # Compute the content-based filtering predictions\n","    content_preds = cosine_sim[business_idx]\n","    \n","    # Combine the predictions from both models\n","    hybrid_pred = (1 - alpha) * collab_pred + alpha * content_preds\n","    \n","    # Get the top 10 recommended business names and their predicted ratings\n","    hybrid_preds = list(enumerate(hybrid_pred))\n","    hybrid_preds = sorted(hybrid_preds, key=lambda x: x[1], reverse=True)\n","    top_business_indices = [x[0] for x in hybrid_preds[:10]]\n","    top_business_names = df.iloc[top_business_indices]['name'].tolist()\n","    top_business_ratings = [x[1] for x in hybrid_preds[:10]]\n","    \n","    return"]},{"cell_type":"markdown","metadata":{"id":"3Hgle7VEj08v"},"source":["### Challenge\n","\n","Cannot set the city"]}],"metadata":{"colab":{"provenance":[{"file_id":"1ZUudk-V0Us9rzRSXm48nhIdDgvbmLZ-T","timestamp":1678874264527}],"authorship_tag":"ABX9TyMMi0eT8eCYFf1Bfdp915ez"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}